#!/usr/bin/env python3

import argparse
import csv
import itertools
import locale
import math
import operator
import re
import sys

INSULT_LEMMES = [
    'bicot',
    'boche',
    'con',
    'connard',
    'connasse',
    'enculé',
    'fifi',
    'fritz',
    'gouine',
    'négro',
    'pédé',
    'putain',
    'pute',
    'salaud',
    'salopard',
    'salope',
    'youpin',
]
VULGAR_LEMMES = [
    'baiseur',
    'bite',
    'burne',
    'chier',
    'couille',
    'cul',
    'emmerde',
    'emmerder',
    'enculer',
    'foutre',
    'merde',
    'merder',
    'merdeux',
    'niquer',
]

def load_lexi_file(file, min_letters=0, max_letters=100, valid_chars='.',
                    skip_lemmes=[], skip_cgrams=[], min_freq=0.0,
                    skip_low_lemmes=True, non_ascii_cost=0):
    data = []
    reader = csv.DictReader(file, delimiter='\t', quoting=csv.QUOTE_NONE)

    trtable = str.maketrans('îû', 'iu')

    for row in reader:
        word = row['1_ortho']
        lemme = row['3_lemme']
        cgram = row['4_cgram']
        freqlemfilm = float(row['7_freqlemfilms2'])
        freqlembook = float(row['8_freqlemlivres'])
        freqfilm = float(row['9_freqfilms2'])
        freqbook = float(row['10_freqlivres'])
        infover = row['11_infover']
        nbletters = int(row['15_nblettres'])

        if nbletters < min_letters or nbletters > max_letters:
            continue

        if lemme in skip_lemmes:
            continue

        if cgram in skip_cgrams:
            continue

        # Application sauvage de la réforme 1990 sur l'accent circonflexe
        if not 'sub:imp' in infover and not 'ind:pas' in infover:
            word = word.translate(trtable)
            lemme = lemme.translate(trtable)

        if re.fullmatch(valid_chars + '+', word) is None:
            continue

        if non_ascii_cost:
            cost = nbletters + len(re.findall('[^a-z]', word)) * non_ascii_cost
            if cost > max_letters:
                continue

        if (skip_low_lemmes and
                (freqlemfilm <= 0.01 or freqlembook <= 0.01)):
            continue

        freq = (2*freqfilm + freqbook) / 3
        if freq < min_freq:
            continue

        freqlem = (2*freqlemfilm + freqlembook) / 3

        data.append({
            'cgram': cgram,
            'freq': freq,
            'freqlem': freqlem,
            'lemme': lemme,
            'mot': word,
            'nbletters': nbletters,
            'phon': row['2_phon'],
        })
        
    return data


def clean_lemmes(data, max_lemme_variants=None):
    """Pour chaque (lemme, cgram), on conserve un seul homophone
    le lemme lui-meme ou la forme la plus frequente de l'homophone"""

    new_data = []

    data.sort(key=operator.itemgetter('lemme', 'cgram'))
    for lemme, words in itertools.groupby(data, key=operator.itemgetter('lemme', 'cgram')):
        words = list(words)

        new_words = []
        words.sort(key=operator.itemgetter('phon'))
        for phon, homophones in itertools.groupby(words, key=operator.itemgetter('phon')):
            homophones = list(homophones)
            homophones.sort(key=lambda x: (x['freq'], -x['nbletters']), reverse=True)
            selected = homophones.pop(0)
            
            # prefer canonic form
            canonic = next((x for x in homophones if x['mot'] == selected['lemme']), None)
            if canonic:
                canonic['freq'] = selected['freq']
                selected = canonic
                
            new_words.append(selected)
        
        new_words.sort(key=operator.itemgetter('freq'), reverse=True)
        new_data.extend(new_words[:max_lemme_variants])

    return new_data


def clean_homophones(data):
    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_phon = []
    known_graph = []
    new_data = []

    for word in data:
        if word['phon'] in known_phon:
            continue
        if word['mot'] in known_graph:
            continue

        new_data.append(word)
        known_phon.append(word['phon'])
        known_graph.append(word['mot'])
        
    return new_data

def select_typo_resistance(data, prefix_size=None, min_dist=None):
    if min_dist:
        try:
            from Levenshtein import distance
        except ImportError:
            min_dist = None
            print(
                'Warning: Could not import Levenshtein (missing '
                'python-levenshtein?), ignoring edit distance requirement',
                file=sys.stderr)

    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_pref = []
    new_data = []
    for word in data:
        if prefix_size:
            pref = word['mot'][:prefix_size]
            if pref in known_pref:
                continue

        if min_dist:
            is_ok = True
            for w in new_data:
                if distance(w['mot'], word['mot']) < min_dist:
                    is_ok = False
                    break

            if not is_ok:
                continue

        if prefix_size:
            known_pref.append(pref)

        new_data.append(word)

    return new_data
    
def print_diceware_list(words, outfile, limit=None, nb_dices=None, nb_sides=6, print_dices=True):
    if nb_dices:
        limit = nb_sides**nb_dices
        if len(words) < limit:
            print('Warning: not enough words', file=sys.stderr)

    if limit:
        words.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
        words = words[:limit]

    nb_words = len(words)

    if print_dices:
        if not nb_dices:
            nb_dices = math.ceil(math.log(nb_words, nb_sides))
        dices = [1] * nb_dices

    try:
        locale.setlocale(locale.LC_COLLATE, 'fr_FR.UTF-8')
    except Exception as e:
        print('Warning: failed to set LC_COLLATE to fr_FR.UTF-8, sorting with default locale', file=sys.stderr)
    words.sort(key=lambda word: locale.strxfrm(word['mot']))
    
    len_total = 0
    count_nonascii = 0
    for word in words:
        len_total = len_total + len(word['mot'])
        if re.search('[^a-z]', word['mot']):
            count_nonascii = count_nonascii + 1

        if print_dices:
            print('{} {}'.format(''.join(str(dice) for dice in dices), word['mot']), file=outfile)

            for x in range(1, nb_dices + 1):
                if dices[-x] < nb_sides:
                    dices[-x] = dices[-x]+1
                    break
                else:
                    dices[-x] = 1
        else:
            print(word['mot'], file=outfile)

    avg_len = len_total / nb_words
    efficiency = round(math.log(nb_words, 2) / avg_len, 1)

    print('{} words ({} non ascii), avg len: {} char., efficiency: ~{} bits / char'.format(
            nb_words, count_nonascii, round(avg_len, 2), efficiency), file=sys.stderr)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('LEXIQUE_FILE', type=argparse.FileType('r'))

    lenargs = parser.add_mutually_exclusive_group()
    lenargs.add_argument('-n', '--nb-dices', type=int, default=5)
    lenargs.add_argument('--limit', type=int)

    parser.add_argument('-c', '--char-regex', default='[a-zàéîôùç]')
    parser.add_argument('-M', '--max-letters', type=int, default=8)
    parser.add_argument('-m', '--min-letters', type=int, default=3)
    parser.add_argument('-a', '--non-ascii-cost', type=int, default=1)
    parser.add_argument('-s', '--skip-cgrams', default='ONO')
    parser.add_argument('-V', '--max-lemme-variants', type=int, default=None)
    parser.add_argument('-X', '--include-bad-words', action='count', default=0)
    parser.add_argument('-p', '--unique-prefix-size', type=int)
    parser.add_argument('-l', '--edit-distance', type=int)
    parser.add_argument('--no-print-dices', action='store_true')
    parser.add_argument('-o', '--out', type=argparse.FileType('w'))

    args = parser.parse_args()

    print_dices = not args.no_print_dices
    skip_cgrams = args.skip_cgrams.split(',')

    nb_dices = args.nb_dices
    if not args.limit is None:
        nb_dices = None

    if args.include_bad_words >= 2:
        skip_lemmes = []
    elif args.include_bad_words == 1:
        skip_lemmes = INSULT_LEMMES
    else:
        skip_lemmes = VULGAR_LEMMES + INSULT_LEMMES

    data = load_lexi_file(args.LEXIQUE_FILE, min_letters=args.min_letters,
                            max_letters=args.max_letters,
                            valid_chars=args.char_regex,
                            skip_lemmes=skip_lemmes, min_freq=0.1,
                            skip_cgrams=skip_cgrams,
                            non_ascii_cost=args.non_ascii_cost)

    data = clean_lemmes(data, max_lemme_variants=args.max_lemme_variants)

    if args.unique_prefix_size or args.edit_distance:
        data = select_typo_resistance(data, args.unique_prefix_size, args.edit_distance)

    data = clean_homophones(data)

    print_diceware_list(data, args.out, limit=args.limit, nb_dices=nb_dices,
                        print_dices=print_dices)
