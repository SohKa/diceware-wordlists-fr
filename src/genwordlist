#!/usr/bin/env python3

import argparse
import csv
import itertools
import locale
import math
import operator
import re
import sys

INSULT_LEMMES = [
    'bicot',
    'boche',
    'con',
    'connard',
    'connasse',
    'enculé',
    'fifi',
    'fritz',
    'gouine',
    'négro',
    'pédé',
    'putain',
    'pute',
    'salaud',
    'salopard',
    'salope',
    'youpin',
]
VULGAR_LEMMES = [
    'bite',
    'burne',
    'chier',
    'couille',
    'cul',
    'emmerde',
    'emmerder',
    'enculer',
    'foutre',
    'merde',
    'merder',
    'merdeux',
    'niquer',
]

def load_lexi_file(file, min_letters=0, max_letters=100, valid_chars='.',
                    skip_lemmes=[], skip_cgrams=[], min_freq=0.0,
                    skip_low_lemmes=True):
    data = []
    reader = csv.DictReader(file, delimiter='\t', quoting=csv.QUOTE_NONE)

    trtable = str.maketrans('îû', 'iu')

    for row in reader:
        word = row['1_ortho']
        lemme = row['3_lemme']
        cgram = row['4_cgram']
        freqlemfilm = float(row['7_freqlemfilms2'])
        freqlembook = float(row['8_freqlemlivres'])
        freqfilm = float(row['9_freqfilms2'])
        freqbook = float(row['10_freqlivres'])
        infover = row['11_infover']
        nbletters = int(row['15_nblettres'])
        
        if nbletters < min_letters or nbletters > max_letters:
            continue

        if lemme in skip_lemmes:
            continue

        if cgram in skip_cgrams:
            continue

        # Application sauvage de la réforme 1990 sur l'accent circonflexe
        if not 'sub:imp' in infover and not 'ind:pas' in infover:
            word = word.translate(trtable)
            lemme = lemme.translate(trtable)

        if re.fullmatch(valid_chars + '+', word) is None:
            continue

        if (skip_low_lemmes and
                (freqlemfilm <= 0.01 or freqlembook <= 0.01)):
            continue

        freq = (2*freqfilm + freqbook) / 3
        if freq < min_freq:
            continue

        freqlem = (2*freqlemfilm + freqlembook) / 3

        data.append({
            'cgram': cgram,
            'freq': freq,
            'freqlem': freqlem,
            'lemme': lemme,
            'mot': word,
            'nbletters': nbletters,
            'phon': row['2_phon'],
        })
        
    return data


def clean_lemmes(data, max_lemme_variants=None):
    """Pour chaque (lemme, cgram), on conserve un seul homophone
    le lemme lui-meme ou la forme la plus frequente de l'homophone"""

    new_data = []

    data.sort(key=operator.itemgetter('lemme', 'cgram'))
    for lemme, words in itertools.groupby(data, key=operator.itemgetter('lemme', 'cgram')):
        words = list(words)

        new_words = []
        words.sort(key=operator.itemgetter('phon'))
        for phon, homophones in itertools.groupby(words, key=operator.itemgetter('phon')):
            homophones = list(homophones)
            homophones.sort(key=lambda x: (x['freq'], -x['nbletters']), reverse=True)
            selected = homophones.pop(0)
            
            # prefer canonic form
            canonic = next((x for x in homophones if x['mot'] == selected['lemme']), None)
            if canonic:
                canonic['freq'] = selected['freq']
                selected = canonic
                
            new_words.append(selected)
        
        new_words.sort(key=operator.itemgetter('freq'), reverse=True)
        new_data.extend(new_words[:max_lemme_variants])

    return new_data


def clean_homophones(data):
    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_phon = []
    known_graph = []
    new_data = []

    for word in data:
        if word['phon'] in known_phon:
            continue
        if word['mot'] in known_graph:
            continue

        new_data.append(word)
        known_phon.append(word['phon'])
        known_graph.append(word['mot'])
        
    return new_data

def clean_prefix(data, prefix_size):
    data.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
    known_pref = []
    new_data = []

    for word in data:
        pref = word['mot'][:prefix_size]
        if pref in known_pref:
            continue

        new_data.append(word)
        known_pref.append(pref)

    return new_data
    
def print_diceware_list(words, outfile, print_dices=True, nb_dices=5, nb_sides=6):
    if nb_dices:
        dices = [1] * nb_dices
        words.sort(key=operator.itemgetter('freq', 'freqlem'), reverse=True)
        words = words[:nb_sides**nb_dices]
    else:
        print_dices = False

    nb_words = len(words)

    try:
        locale.setlocale(locale.LC_COLLATE, 'fr_FR.UTF-8')
    except Exception as e:
        print('warning: failed to set LC_COLLATE to fr_FR.UTF-8, sorting with default locale', file=sys.stderr)
    words.sort(key=lambda word: locale.strxfrm(word['mot']))
    
    len_total = 0
    count_nonascii = 0
    for word in words:
        len_total = len_total + len(word['mot'])
        if re.search('[^a-z]', word['mot']):
            count_nonascii = count_nonascii + 1

        if print_dices:
            print('{} {}'.format(''.join(str(dice) for dice in dices), word['mot']), file=outfile)

            for x in range(1, nb_dices + 1):
                if dices[-x] < nb_sides:
                    dices[-x] = dices[-x]+1
                    break
                else:
                    dices[-x] = 1
        else:
            print(word['mot'], file=outfile)

    avg_len = len_total / nb_words
    efficiency = round(math.log(nb_words, 2) / avg_len, 1)

    print('{} words ({} non ascii), avg len: {} char., efficiency: ~{} bits / char'.format(
            nb_words, count_nonascii, round(avg_len, 2), efficiency), file=sys.stderr)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('LEXIQUE_FILE', type=argparse.FileType('r'))

    parser.add_argument('-c', '--char-regex', default='[a-zàéîôù]')
    parser.add_argument('-M', '--max-letters', type=int, default=8)
    parser.add_argument('-m', '--min-letters', type=int, default=3)
    parser.add_argument('-n', '--nb-dices', type=int, default=5)
    parser.add_argument('--skip-cgrams', default='ONO')
    parser.add_argument('--max-lemme-variants', type=int, default=None)
    parser.add_argument('-X', '--include-bad-words', action='count', default=0)
    parser.add_argument('--unique-prefix', type=int)
    parser.add_argument('--no-print-dices', action='store_true')
    parser.add_argument('-o', '--out', type=argparse.FileType('w'))

    args = parser.parse_args()

    min_letters = args.min_letters
    max_letters = args.max_letters
    nb_dices = args.nb_dices
    valid_chars = args.char_regex
    print_dices = not args.no_print_dices
    skip_cgrams = args.skip_cgrams.split(',')
    max_lemme_variants = args.max_lemme_variants
    prefix = args.unique_prefix

    if args.include_bad_words >= 2:
        skip_lemmes = []
    elif args.include_bad_words == 1:
        skip_lemmes = INSULT_LEMMES
    else:
        skip_lemmes = VULGAR_LEMMES + INSULT_LEMMES

    data = load_lexi_file(args.LEXIQUE_FILE, min_letters=min_letters,
                            max_letters=max_letters, valid_chars=valid_chars,
                            skip_lemmes=skip_lemmes, min_freq=0.1,
                            skip_cgrams=skip_cgrams)

    data = clean_lemmes(data, max_lemme_variants=max_lemme_variants)
    if prefix:
        data = clean_prefix(data, prefix)
    data = clean_homophones(data)

    print_diceware_list(data, args.out, nb_dices=nb_dices, print_dices=print_dices)
